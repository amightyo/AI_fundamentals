{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Copy your Text Analytics Key `cognitive_key` and Endpoint `cognitive_endpoint` to the variables listed below."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cognitive_key = '<YourKey>'\r\n",
        "cognitive_endpoint = '<YourEndPoint>'\r\n",
        "\r\n",
        "print('Cognitive services ready at {}'.format(cognitive_key))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cognitive services ready at e2df3b1e2c6b4c0ea62c8dc03b725d3b\n"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1613914109529
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the code below to install Azure Text Analytics SDK to the exercise environment."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install azure-ai-textanalytics"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: azure-ai-textanalytics in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (5.0.0)\n",
            "Requirement already satisfied: msrest>=0.6.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azure-ai-textanalytics) (0.6.19)\n",
            "Requirement already satisfied: azure-core<2.0.0,>=1.4.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azure-ai-textanalytics) (1.9.0)\n",
            "Requirement already satisfied: azure-common~=1.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azure-ai-textanalytics) (1.1.26)\n",
            "Requirement already satisfied: six>=1.6 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azure-ai-textanalytics) (1.15.0)\n",
            "Requirement already satisfied: requests~=2.16 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from msrest>=0.6.0->azure-ai-textanalytics) (2.25.1)\n",
            "Requirement already satisfied: isodate>=0.6.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from msrest>=0.6.0->azure-ai-textanalytics) (0.6.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.5.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from msrest>=0.6.0->azure-ai-textanalytics) (1.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from msrest>=0.6.0->azure-ai-textanalytics) (2020.12.5)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests~=2.16->msrest>=0.6.0->azure-ai-textanalytics) (1.25.11)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests~=2.16->msrest>=0.6.0->azure-ai-textanalytics) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests~=2.16->msrest>=0.6.0->azure-ai-textanalytics) (4.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests-oauthlib>=0.5.0->msrest>=0.6.0->azure-ai-textanalytics) (3.1.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1613914113829
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Time to create a `TextAnalyticsClient` that we can use to access the REST APIs hosted by the Azure Text Analytics service. We will be using this client for all the upcoming operations."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.core.credentials import AzureKeyCredential\r\n",
        "from azure.ai.textanalytics import TextAnalyticsClient\r\n",
        "\r\n",
        "credential = AzureKeyCredential(cognitive_key)\r\n",
        "text_analytics_client = TextAnalyticsClient(endpoint=cognitive_endpoint, credential=credential)"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1613914114115
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Language detection\r\n",
        "\r\n",
        "Here is how we do language detection for some predefined text. We call `detect_language` method from `text_analytics_client` and pass all the documents we have as a parameter. The result is an array of documents where we can access the language name and the confidence score."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [\r\n",
        "    \"This is written in English.\",\r\n",
        "    \"Ceci est écrit en Français.\",\r\n",
        "    \"Dies ist in deutscher Sprache geschrieben.\"\r\n",
        "]\r\n",
        "\r\n",
        "language_analysis = text_analytics_client.detect_language(documents)\r\n",
        "result = [doc for doc in language_analysis if not doc.is_error]\r\n",
        "\r\n",
        "for doc in result:\r\n",
        "    print(\"Language detected: {}\".format(doc.primary_language.name))\r\n",
        "    print(\"ISO6391 name: {}\".format(doc.primary_language.iso6391_name))\r\n",
        "    print(\"Confidence score: {}\\n\".format(doc.primary_language.confidence_score))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Language detected: English\n",
            "ISO6391 name: en\n",
            "Confidence score: 1.0\n",
            "\n",
            "Language detected: French\n",
            "ISO6391 name: fr\n",
            "Confidence score: 1.0\n",
            "\n",
            "Language detected: German\n",
            "ISO6391 name: de\n",
            "Confidence score: 1.0\n",
            "\n"
          ]
        }
      ],
      "execution_count": 4,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1613914114759
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sentiment Analysis\r\n",
        "\r\n",
        "To run sentiment analysis for our documents stored as an array, we will call `analyze_sentiment` from the `text_analytics_client` instance we have. Our first parameter contains the list of documents, and the second is the language used in the documents. We get a result of an array of documents that give us positivity layers and the confidence score for each sentiment."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [\r\n",
        "    \"I'm in love with Azure Text Analytics\",\r\n",
        "    \"AI is going to eat our jobs. We will all end up homeless.\",\r\n",
        "    \"Machine Learning skills are always valuable.\"\r\n",
        "]\r\n",
        "\r\n",
        "response = text_analytics_client.analyze_sentiment(documents, language=\"en\")\r\n",
        "result = [doc for doc in response if not doc.is_error]\r\n",
        "\r\n",
        "for doc in result:\r\n",
        "    print(\"Overall sentiment: {}\".format(doc.sentiment))\r\n",
        "    print(\"Scores: positive={}; neutral={}; negative={} \\n\".format(\r\n",
        "        doc.confidence_scores.positive,\r\n",
        "        doc.confidence_scores.neutral,\r\n",
        "        doc.confidence_scores.negative,\r\n",
        "    ))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall sentiment: positive\n",
            "Scores: positive=1.0; neutral=0.0; negative=0.0 \n",
            "\n",
            "Overall sentiment: negative\n",
            "Scores: positive=0.02; neutral=0.33; negative=0.65 \n",
            "\n",
            "Overall sentiment: positive\n",
            "Scores: positive=1.0; neutral=0.0; negative=0.0 \n",
            "\n"
          ]
        }
      ],
      "execution_count": 5,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1613914114964
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Key Phrase Extraction\r\n",
        "Key phrase detection comes with the `extract_key_phrases` method of `text_analytics_client` instance that we have. Our first parameter contains the list of documents, and the second is the language used in the documents. We get a result of an array of documents that give us an array of key phrases for every document we have submitted."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [\r\n",
        "    \"Udacity is the perfect place to learn by practice and exercise.\",\r\n",
        "    \"I might eat a burger tonight.\",\r\n",
        "    \"The answer to everything in life is 42.\"\r\n",
        "]\r\n",
        "\r\n",
        "response = text_analytics_client.extract_key_phrases(documents, language=\"en\")\r\n",
        "result = [doc for doc in response if not doc.is_error]\r\n",
        "\r\n",
        "for doc in result:\r\n",
        "    print(doc.key_phrases)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['perfect place', 'practice', 'Udacity', 'exercise']\n",
            "['burger']\n",
            "['answer', 'life']\n"
          ]
        }
      ],
      "execution_count": 6,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1613914115115
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entity Recognition\r\n",
        "To implement Entity Recognition for a text we will use `recognize_entities` from our `text_analytics_client` instance. Our first parameter contains the list of documents, and the second is the language used in the documents. We get a result of an array of documents that give us an array of entities recognized for every document we have submitted."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [\r\n",
        "    \"David woke up very early.\",\r\n",
        "    \"When she was in Paris, Diana was a happy girl.\",\r\n",
        "    \"He found a job thanks to Udacity.\"\r\n",
        "]\r\n",
        "\r\n",
        "response = text_analytics_client.recognize_entities(documents, language=\"en\")\r\n",
        "result = [doc for doc in response if not doc.is_error]\r\n",
        "\r\n",
        "for doc in result:\r\n",
        "    for entity in doc.entities:\r\n",
        "        print(\"Entity: \\t\", entity.text, \"\\tCategory: \\t\", entity.category,\r\n",
        "              \"\\tConfidence Score: \\t\", entity.confidence_score)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entity: \t David \tCategory: \t Person \tConfidence Score: \t 0.95\n",
            "Entity: \t Paris \tCategory: \t Location \tConfidence Score: \t 0.99\n",
            "Entity: \t Diana \tCategory: \t Person \tConfidence Score: \t 0.98\n",
            "Entity: \t Udacity \tCategory: \t Organization \tConfidence Score: \t 0.84\n"
          ]
        }
      ],
      "execution_count": 8,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1613914121694
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}