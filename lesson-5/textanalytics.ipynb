{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Copy your Text Analytics Key `cognitive_key` and Endpoint `cognitive_endpoint` to the variables listed below."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cognitive_key = '<YourKey>'\r\n",
        "cognitive_endpoint = '<YourEndpoint>'\r\n",
        "\r\n",
        "print('Cognitive services ready at {}'.format(cognitive_key))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cognitive services ready at e2df3b1e2c6b4c0ea62c8dc03b725d3b\n"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1613679794890
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the code below to install Azure Text Analytics SDK to the exercise environment."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install azure-ai-textanalytics"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: azure-ai-textanalytics in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (5.0.0)\n",
            "Requirement already satisfied: azure-core<2.0.0,>=1.4.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azure-ai-textanalytics) (1.9.0)\n",
            "Requirement already satisfied: azure-common~=1.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azure-ai-textanalytics) (1.1.26)\n",
            "Requirement already satisfied: msrest>=0.6.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azure-ai-textanalytics) (0.6.19)\n",
            "Requirement already satisfied: six>=1.6 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azure-ai-textanalytics) (1.15.0)\n",
            "Requirement already satisfied: requests>=2.18.4 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azure-core<2.0.0,>=1.4.0->azure-ai-textanalytics) (2.25.1)\n",
            "Requirement already satisfied: isodate>=0.6.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from msrest>=0.6.0->azure-ai-textanalytics) (0.6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from msrest>=0.6.0->azure-ai-textanalytics) (2020.12.5)\n",
            "Requirement already satisfied: requests-oauthlib>=0.5.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from msrest>=0.6.0->azure-ai-textanalytics) (1.3.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.4.0->azure-ai-textanalytics) (1.25.11)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.4.0->azure-ai-textanalytics) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.4.0->azure-ai-textanalytics) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests-oauthlib>=0.5.0->msrest>=0.6.0->azure-ai-textanalytics) (3.1.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1613679797720
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Time to create a `TextAnalyticsClient` that we can use to access the REST APIs hosted by the Azure Text Analytics service. We will be using this client for all the upcoming operations."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.core.credentials import AzureKeyCredential\r\n",
        "from azure.ai.textanalytics import TextAnalyticsClient\r\n",
        "\r\n",
        "credential = AzureKeyCredential(cognitive_key)\r\n",
        "text_analytics_client = TextAnalyticsClient(endpoint=cognitive_endpoint, credential=credential)"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1613679797981
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Language detection\r\n",
        "\r\n",
        "Here you have a list of texts written in a different language. Use Azure Cognitive Service's language detection capability to detect and output the languages used in the documents. A sample result is given below."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [\r\n",
        "    \"This is written in English.\",\r\n",
        "    \"Ceci est écrit en Français.\",\r\n",
        "    \"Dies ist in deutscher Sprache geschrieben.\"\r\n",
        "]"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Language detected: English\n",
            "ISO6391 name: en\n",
            "Confidence score: 1.0\n",
            "\n",
            "Language detected: French\n",
            "ISO6391 name: fr\n",
            "Confidence score: 1.0\n",
            "\n",
            "Language detected: German\n",
            "ISO6391 name: de\n",
            "Confidence score: 1.0\n",
            "\n"
          ]
        }
      ],
      "execution_count": 4,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1613679799175
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\r\n",
        "Language detected: English\r\n",
        "ISO6391 name: en\r\n",
        "Confidence score: 1.0\r\n",
        "\r\n",
        "Language detected: French\r\n",
        "ISO6391 name: fr\r\n",
        "Confidence score: 1.0\r\n",
        "\r\n",
        "Language detected: German\r\n",
        "ISO6391 name: de\r\n",
        "Confidence score: 1.0\r\n",
        "```"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sentiment Analysis\r\n",
        "\r\n",
        "Sentiment analysis can be fun runnings against some tweets. Below you have a small list of tweets. Run Azure Cognitive Services Sentiment Analysis to see what AI thinks about these tweets. Make sure you print both the sentiment and the confidence levels. \r\n",
        "You can see a sample result below."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [\r\n",
        "    \"I'm in love with Azure Text Analytics\",\r\n",
        "    \"AI is going to eat our jobs. We will all end up homeless.\",\r\n",
        "    \"Machine Learning skills are always valuable.\"\r\n",
        "]"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall sentiment: positive\n",
            "Scores: positive=1.0; neutral=0.0; negative=0.0 \n",
            "\n",
            "Overall sentiment: negative\n",
            "Scores: positive=0.02; neutral=0.33; negative=0.65 \n",
            "\n",
            "Overall sentiment: positive\n",
            "Scores: positive=1.0; neutral=0.0; negative=0.0 \n",
            "\n"
          ]
        }
      ],
      "execution_count": 5,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1613679799474
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\r\n",
        "Overall sentiment: positive\r\n",
        "Scores: positive=1.0; neutral=0.0; negative=0.0 \r\n",
        "\r\n",
        "Overall sentiment: negative\r\n",
        "Scores: positive=0.02; neutral=0.33; negative=0.65 \r\n",
        "\r\n",
        "Overall sentiment: positive\r\n",
        "Scores: positive=1.0; neutral=0.0; negative=0.0 \r\n",
        "```"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Key Phrase Extraction\r\n",
        "\r\n",
        "We have another list of tweets. Now we want to extract key phrases from each document and output those in a list. How would you do it? See the sample output given below."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [\r\n",
        "    \"Udacity is the perfect place to learn by practice and exercise.\",\r\n",
        "    \"I might eat a burger tonight.\",\r\n",
        "    \"The answer to everything in life is 42.\"\r\n",
        "]"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['perfect place', 'practice', 'Udacity', 'exercise']\n",
            "['burger']\n",
            "['answer', 'life']\n"
          ]
        }
      ],
      "execution_count": 6,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1613679799694
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\r\n",
        "['perfect place', 'practice', 'Udacity', 'exercise']\r\n",
        "['burger']\r\n",
        "['answer', 'life']\r\n",
        "```"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entity Recognition\r\n",
        "\r\n",
        "Recognizing entities in a text can be very valuable to extract actionable information from text. Below we have a list of documents to run through the Entity Recognizer in Azure Cognitive Services. See the sample output and try to match it."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [\r\n",
        "    \"David woke up very early.\",\r\n",
        "    \"When she was in Paris, Diana was a happy girl.\",\r\n",
        "    \"He found a job thanks to Udacity.\"\r\n",
        "]"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entity: \t David \tCategory: \t Person \tConfidence Score: \t 0.95\n",
            "Entity: \t Paris \tCategory: \t Location \tConfidence Score: \t 0.99\n",
            "Entity: \t Diana \tCategory: \t Person \tConfidence Score: \t 0.98\n",
            "Entity: \t Udacity \tCategory: \t Organization \tConfidence Score: \t 0.84\n"
          ]
        }
      ],
      "execution_count": 8,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1613679808312
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\r\n",
        "\r\n",
        "Entity: \t David \tCategory: \t Person \tConfidence Score: \t 0.95\r\n",
        "Entity: \t Paris \tCategory: \t Location \tConfidence Score: \t 0.99\r\n",
        "Entity: \t Diana \tCategory: \t Person \tConfidence Score: \t 0.98\r\n",
        "Entity: \t Udacity \tCategory: \t Organization \tConfidence Score: \t 0.84\r\n",
        "```"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}